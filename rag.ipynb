{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a865dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain import hub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd78f71",
   "metadata": {},
   "source": [
    "Import pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35a8c431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Pages: 20\n"
     ]
    }
   ],
   "source": [
    "Loader = PyPDFLoader\n",
    "FILE_PATH = 'D:/AIO/Module 1/ProjectRAG/test.pdf'\n",
    "loader = Loader(FILE_PATH)\n",
    "documents = loader.load()\n",
    "print(f\"Number of Pages: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326bb0f8",
   "metadata": {},
   "source": [
    "Create embedding model for executing Vietnamese "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bd9db38",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name = \"bkai-foundation-models/vietnamese-bi-encoder\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8680fa3f",
   "metadata": {},
   "source": [
    "Text splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3472b79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_splitter = SemanticChunker(\n",
    "    embeddings=embedding,\n",
    "    buffer_size=1,\n",
    "    breakpoint_threshold_type=\"percentile\",\n",
    "    breakpoint_threshold_amount=95,\n",
    "    min_chunk_size=500,\n",
    "    add_start_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "734634e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of semantic chunks:  32\n"
     ]
    }
   ],
   "source": [
    "docs = semantic_splitter.split_documents(documents)\n",
    "print('Number of semantic chunks: ', len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4986bbac",
   "metadata": {},
   "source": [
    "Create database vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd824820",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = Chroma.from_documents(documents=docs,\n",
    "                                  embedding=embedding)\n",
    "retriever = vector_db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bffb4159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of revelant documents:  4\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "result = retriever.invoke('What is YOLO?')\n",
    "print(\"Number of revelant documents: \", len(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58331abb",
   "metadata": {},
   "source": [
    "Use Vicuna-7b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9eac8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0987b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|██████████| 2/2 [19:34<00:00, 587.38s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:21<00:00, 10.57s/it]\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "# model and tokenizer\n",
    "MODEL_NAME = 'lmsys/vicuna-7b-v1.5'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config = nf4_config,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "441615e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=model_pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1e0e82",
   "metadata": {},
   "source": [
    "Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49b2fe31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langsmith\\client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv10 là một phiên bản mới của hệ thống dự đoán đối tượng (Object Detection) được giới thiệu vào năm 2024 bởi Chien-Yao Wang, I-Hau Yeh, và Hong-Yuan Mark Liao. Mô hình này cải thiện độ chính xác và tốc độ so với YOLOv8 và giới thiệu nhiều kỹ thuật mới như Programmable Gradient Information (PGI) và Generalized Efficient Layer Aggregation Network (GELAN). YOLOv10 đạt được kết quả tốt nhất về khía cạnh Độ trễ (Latency) và Số lượng tham số mô hình (Number of parameters) trong khi vẫn giữ được độ chính xác (COCO AP) cao.\n"
     ]
    }
   ],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "USER_QUESTION = \"YOLOv10 là gì?\"\n",
    "output = rag_chain.invoke(USER_QUESTION)\n",
    "answer = output.split(\"Answer:\")[1].strip()\n",
    "\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
